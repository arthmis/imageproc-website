<!DOCTYPE html>
<html>

    <head>
        <title>Image Processing</title>
        <meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.5, user-scalable=no">
        <link rel="stylesheet" type = "text/css" href="main.css"/>
        <link href="https://fonts.googleapis.com/css?family=Open+Sans&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="fonts/font-awesome-4.7.0/css/font-awesome.min.css">

    </head>

    <body>
        <nav class="navbar">
            <a href="index.html">
                <i class="fa fa-home" aria-hidden="true" style="font-size:2em;"></i>
            </a>
            
            <a href="https://github.com/lazypassion/imageproc-website">
                <i class="fa fa-github" aria-hidden="true" style="font-size:1.4em;"></i>
                Github
            </a>
        </nav>
        <div id="upload-image-algorithms">
            <div id="open-algorithms-wrapper">
                <button id="open-algorithms">
                    <i id="open-algorithms-arrow" class="fa fa-angle-right" aria-hidden="true" style="font-size:1.2em;"></i>
                    Algorithms
                </button>
            </div>
            <div id="input-image"> 
                <input type="file" id="file-input" accept="image/png, image/jpeg" style="display: none;">
                <button id="upload-image">
                    <i class="fa fa-upload" aria-hidden="true" style="font-size:1em;"></i>
                    Upload Image
                </button>
            </div>
        </div>
        <main>
            <div id="main-content">
                <div id="sidebar" class="mobile-hidden">
                    <div id="options">
                        <ul class="processing-category"><span>Algorithms</span>
                            <li id="invert-option" class="sidebar-elem sidebar-first-elem">Invert</li>
                            <li id="gamma-option" class="sidebar-elem">Gamma</li>
                            <li id="box-blur-option" class="sidebar-elem">Box Blur</li>
                            <li id="sobel-option" class="sidebar-elem">Sobel Edge Detector</li>
                        </ul>
                    </div>
                </div>
                <div id="image-options">
                    <div id="canvas-wrapper">
                        <button id="large-upload-button">
                            <i class="fa fa-upload large-button" aria-hidden="true"></i>
                            <span class="large-button">Upload Image</span> 
                        </button>
                        <canvas id="input-canvas"></canvas>
                    </div>
                    <div id="processing-options" style="display: none;">
                        <div id="invert-button" class="algorithm-button-wrapper algorithm-input-styling" style="display:none;">
                            <button >Invert</button>
                        </div>
                        <div id="box-blur-slider-wrapper" class="algorithm-input-styling" style="display: none;">
                            <div class="slider-value">
                                <label for=box-blur-value">blur diameter</label>
                                <input id="box-blur-value" type="text" name="box-blur-value" readonly>
                            </div>
                            <div class="slider">
                                <input class="slider-style" id="box-blur-slider" type="range" name="box-blur" min="1" max="99" step="2">
                            </div>
                        </div>
                        <div id="gamma-slider-wrapper" class="algorithm-input-styling" style="display: none;">
                            <div class="slider-value">
                                <label for="gamma-value">Gamma</label>
                                <input id="gamma-value" type="text" name="gamma-value" readonly>
                            </div>
                            <div class="slider">
                                <input class="slider-style" id="gamma-slider" type="range" name="gamma" min="0.1" max="5" step="0.01">
                            </div>
                        </div>
                        <div id="sobel-slider-wrapper" class="algorithm-input-styling" style="display: none;">
                            <div class="slider-value">
                                <label for="sobel-threshold">Threshold</label>
                                <input id="sobel-threshold" type="text" name="sobel-threshold" readonly>
                            </div>
                            <div class="slider">
                                <input class="slider-style" id="sobel-slider" type="range" name="sobel" min="1" max="255" step="1">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="algorithm-info">
                <div id="invert-info" class="algorithm-info algorithm-explanation" style="display: none;">
                    <h2>How Invert works</h2>
                    <p>
                        Color inversion is when the luminance of the pixel gets mapped to the opposite.
                        In other words, white gets mapped to black and black is mapped to white. If we assume
                        an image with the color channels red, green, blue and 8 bits per channel, then
                        the formula to map the luminance value is given by:
                        <p class="formula">
                            <i>I<sub>invert</sub>(x, y) = Luminance<sub>max</sub> - I(x, y)</i>
                        </p>
                        where <i>I</i> is the image, <i>Luminance<sub>max</sub></i> is 255 for an image with 8 bits
                        per color channel, and <i>x</i> and <i>y</i> are the coordinates of the pixel.
                        Using the above formula, if you have the RGB pixel <i>I(x, y)</i> = (50, 120, 200)
                        then <i>I<sub>invert</sub>(x, y)</i> = (205, 135, 55).
                    </p>
                </div>
                <div id="box-blur-info" class="algorithm-info algorithm-explanation" style="display: none;">
                    <h2>How Box Blur works</h2>
                    <p>
                        Box blur is a <a href="https://en.wikipedia.org/wiki/Convolution">convolution filter</a> that blurs an image. 
                        It is also known as a mean filter. What it essentially does is take the average of the pixel
                        it is centered on and all of the neighboring pixels within the filter's radius. Then it 
                        uses the averaged value for the blurred image.
                    </p>
                    <p>
                        The <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel</a> used 
                        for the box blur convolution filter in image processing comes in the form of an
                        <i>nxn</i> matrix divided by the scalar value of <i>n * n</i> where <i>n</i> is the diameter 
                        of the kernel in pixels. For example: 
                    </p>
                    <div class="diagram">
                        <img src="images/box-blur.png" class="diagram-image" alt="box blur kernel">
                        <p>
                            <i>3x3</i> Box Blur Kernel 
                        </p>
                    </div>
                    <p>
                        The above kernel is 3 pixels wide by 3 pixels high. Each square overlaps a pixel on the image.
                        The kernel can be any diameter from 1 to <span style="font-size: 1.5em;">&infin;</span>;
                        larger kernels create a stronger blur. They are odd in size because they need to be centered
                        on a pixel.
                    </p>
                    <p>Heres a demo of how box blur kernel convolves with an image: </p>
                    <div class="diagram">
                        <img class="diagram-image" src="./images/box-blur.gif" alt="box blur animation">
                        <p>
                            Short demo of <i>3x3</i> box blur. The right matrix is the original image
                            and the left matrix is the result of the convolution of the box blur kernel
                            and the right image.                                 
                        </p>
                    </div>
                    <p>
                        The kernel starts at the first pixel of the image at the 
                        top left corner, with the value of 25. It continues by sliding to the right
                        and calculating the average of each pixel within the kernel window at each pixel
                        until the end of the first row then moves down to the next row.
                    </p>
                    <footer>
                        <h2>Sources and Related Reading</h2>
                        <p><a href="https://en.wikipedia.org/wiki/Box_blur">Box Blur Wiki</a></p>
                    </footer>
                </div>
                <div id="gamma-info" class="algorithm-info algorithm-explanation" style="display: none;">
                    <h2>How Gamma works</h2>
                    <p>
                        Gamma, otherwise known as gamma encoding and decoding, or gamma correction, refers to expanding or compressing an image's range 
                        of intensity values. When an imaging system captures an image the light intensity information
                        is captured linearly. In other words, if the light intensity is recorded by the camera
                        with a value of 100, then 200 would be the equivalent of twice the brightness of 100.
                        However, the human vision system perceives light nonlinearly. We are more sensitive to darker tones
                        than lighter tones. Gamma encoding takes advantage of our perception of light intensity by transforming 
                        the linear output of digital imaging devices into a perceptually uniform scale. 
                    </p>
                    <p>
                        An example of perceptual brightness vs linear brightness differences is shown by the diagram below:
                    </p>
                    <div class="diagram">
                        <img class="diagram-image" src="./images/linear_image.jpeg" alt="linear image example">
                        <p>
                            Perceptually linear image
                        </p>
                        <img class="diagram-image" src="./images/gamma_encoded_image.jpeg" alt="gamma encoded image">
                        <p>
                            Linear image
                        </p>
                    </div>
                    <p>
                        The above images show the effect of an encoded image and an image without gamma
                        encoding. 
                        
                        The transformation for gamma encoding is:
                    </p>
                    <p class="formula">
                        <i>I<sub>encoded</sub>(x, y) = I<sub>linear</sub>(x, y)<sup>1/&#947</sup></i>
                        <p style="text-indent: 1em">
                            where &#947 is the gamma value, x and y are the pixel coordinates, and I is the image.
                        </p>
                    </p>
                    <p>
                        The difference in value between each shade for the perceptually linear image 
                        isn't the same; however, the delta between each shade on the linear image is the same.
                        However, the linear image doesn't appear to have a natural progression between each shade
                        while the perceptually linear image does. The difference between each shade looks more uniform in the
                        perceptually linear image because of our nonlinear vision system. When being displayed the monitor and GPU
                        apply another gamma transformation, which is supposed to effectively undo the gamma encoding already applied to image.
                        This is known as gamma decoding. The transformation used for gamma decoding is given by:
                    </p>
                    <p class="formula">
                        <i>I<sub>linear</sub>(x, y) = I<sub>encoded</sub>(x, y)<sup>&#947</sup></i>
                    </p>
                    <p>
                        The monitor expects a gamma to be applied to the image before being displayed. The reason
                        why the perceptually linear image looks uniform is because the gamma applied by the display devices
                        closely approximates the perceptual uniformity the human vision system expects. For most display devices, the gamma 
                        encoding they expect is given by:
                    </p>
                    <p class="formula">
                        <i>I<sub>encoded</sub>(x, y) = I<sub>linear</sub>(x, y)<sup>1/2.2</sup></i>
                    </p>
                    <p>
                        Then to display the image, the monitor will apply the gamma:
                    </p>
                    <p class="formula">
                        <i>I<sub>linear</sub>(x, y) = I<sub>encoded</sub>(x, y)<sup>2.2</sup></i>
                    </p>
                    <p>
                        to decode the image.
                    </p>
                    <h2>Why Gamma is Useful</h2>
                    <p>
                        Generally images are stored using 8 bit <abbr title="red, green, blue">RGB</abbr> channels.
                        Looking at the above linear and perceptually linear image, if 8 bits were used
                        for the linear images, then too many bits would be used to represent lighter values, while
                        too few would be used for darker tones, which humans are more sensitive too. In order to describe
                        more darker tones using linear gamma encoding, one would have to use 11 bits to get sufficient 
                        representation to avoid <a href="https://www.cambridgeincolour.com/tutorials/posterization.htm">image posterization</a>.
                        In other words, 11 bits has to be used in order to get smooth gradients in the image, which are
                        important for things like skies. However, if images are stored using 8 bit gamma encoded values,
                        then it's possible to get smooth gradients using only 8 bits because gamma encoding <q cite="https://www.cambridgeincolour.com/tutorials/gamma-correction.htm">redistributes
                        tonal levels closer to how our eyes perceive them</q>. Gamma encoding helps describe images using fewer
                        bits, thus using less storage and memory. 
                    </p>
                    <footer>
                        <h2>Sources and Related Reading</h2>
                        <p><a href="https://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/">What Every Coder Should Know About Gamma</a></p>
                        <p><a href="https://www.cambridgeincolour.com/tutorials/gamma-correction.htm">Gamma Correction (Cambridge in Color)</a></p>
                        <p><a href="https://en.wikipedia.org/wiki/Gamma_correction">Gamma Correction Wiki</a></p>
                    </footer>
                </div>
                <div id="sobel-info" class="algorithm-info algorithm-explanation" style="display: none;">
                    <h2>Sobel Edge Detector</h2>
                    <p>
                        The Sobel edge detector is one of the more basic edge detectors
                        used in image processing. The goal of an edge detector is to find areas of an image
                        where there are abrupt brightness changes as well as abrupt color changes.
                        Most edge detectors primarily work with brightness changes because it's easier
                        to work with grayscale images, that only show luminance, than color images. Color images
                        require a little more work because two colors might be different and form an edge
                        between each other, but have the same brightness. Which means they wouldn't be detected 
                        by popular edge detectors like the Canny edge detector and Sobel.
                    </p>
                    <p>
                        The Sobel operator finds edges by using two <i>3 x 3</i> <a href="https://en.wikipedia.org/wiki/Convolution">convolution kernels</a> to find the difference
                        between two regions of an image. Finding the edges of an image is broken into two steps. 
                        One operator goes through the image to find the vertical edges and another is used to find
                        horizontal edges. The two kernels are: 
                    </p>
                    <div class="diagram">
                        <img class="diagram-image" src="./images/sobel_operators.png" alt="sobel edge operators">
                        <p>The left kernel finds horizontal edges<br> 
                        The right kernel finds vertical edges</p> 
                    </div>
                    <p>
                        An edge is essentially described as the difference in intensity between two areas.
                        The horizontal edges are found by subtracting the right
                        half of the <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel</a> 
                        from the left half of the kernel as it passes over the image. The vertical edges are found by 
                        subtracting the bottom half of the kernel from the top half of the kernel. 
                        Finally, the two different results for the vertical and horizontal edges are 
                        combined together using the formula
                        <p class="formula">
                            <i>I(x, y) = sqrt((I<sub>h</sub>(x, y))<sup>2</sup> + (I<sub>v</sub>(x, y))<sup>2</sup>)</i>
                        </p>
                        <p>
                            Where I<sub>h</sub> is the horizontal edge image, I<sub>v</sub> is the vertical edge
                            image, x and y are the pixel coordinates and sqrt() is square root.
                        </p>
                    </p>
                    <div class="diagram">
                        <img class="diagram-image" src="images/horizontal_edges.jpeg" alt="horizontal edge image">
                        <p>Horizontal edge image</p>
                    </div>
                    <div class="diagram">
                        <img class="diagram-image" src="images/vertical_edges.jpeg" alt="vertical edge image">
                        <p>Vertical edge image</p>
                    </div>
                    <p>
                        Once the two edge images are consolidated a threshold is applied to the image. The threshold
                        is used to decide whether the difference between two areas is enough to determine
                        that it is a strong edge. Thresholding is useful for preventing image noise from showing up
                        on the final edge image. Lower thresholds will show more edges but will also show more noise while
                        a higher threshold will show less noise but will also show stronger edges. Determining the ideal
                        threshold is dependent on the application and the amount of noise already found in an image.
                        The threshold used for the example assumes an 8 bit image and has a range of 0 to 255. When
                        thresholding an image, if the pixel of the final edge image has the brightness of 200 and 
                        the threshold is 100 then that pixel is usually given max brightness because it's above the threshold. Anything
                        below that threshold is given a brightness of 0. This makes it easier to see the edges. 
                        I went for an inverted thresholding because it looks like a sketch.
                    </p>
                    <div class="diagram">
                        <img class="diagram-image" src="images/final_edges_before_threshold.jpeg" alt="final edge image before thresholding">
                        <p>Final edges before thresholding</p>
                    </div>
                    <div class="diagram">
                        <img class="diagram-image" src="images/final_edges.jpeg" alt="final edge image">
                        <p>Final edges image after thresholding</p>
                    </div>
                    <p>
                        To help with noise control and aid in edge detection, low pass filters or a <a href="https://en.wikipedia.org/wiki/Median_filter">median filter</a> are usually used on 
                        images prior to edge detection. Low pass filters like <a href="https://en.wikipedia.org/wiki/Box_blur">Box blur</a> and <a href="https://en.wikipedia.org/wiki/Gaussian_blur">Gaussian blur</a> can average
                        away noise while maintaining most of the edge information of the image. A median filter are 
                        usually used for noise reduction and make a good preprocessing step before edge detection.
                    </p>
                    <footer>
                        <h2>Sources and Related Reading</h2>
                        <p><a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel Operator - Wiki</a></p>
                    </footer>
                </div>
            </div>
        </main>

        <script type="module" src="./js/dist/index.js"></script>
    </body>
</html>
